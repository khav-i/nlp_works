# nlp_works

|№|Название (ссылка)|Описание|Комментарий|
|-|-|-|-|
|1|[20newsgroups](https://github.com/khav-i/nlp_works/blob/master/20newsgroups/README.md)|Классификация новостей по группам.|Сравниваем fc-сеть с простым перцептроном из sklearn. [Оригинал кода](https://github.com/sic-rus-ai/stepik-dl-nlp/blob/master/task1_20newsgroups.ipynb).|
|2|[Word2Vec](https://github.com/khav-i/nlp_works/blob/master/Word2Vec/README.md)|Трогаем ручками word2vec.|[Оригинал кода](https://github.com/sic-rus-ai/stepik-dl-nlp/blob/master/task2_word_embeddings.ipynb).|
|3|[Twitter sentiment analysis](https://github.com/khav-i/nlp_works/blob/master/Twitter%20sentiment%20analysis/README.md)|Классификация настроений коротких текстов из популярного датасета.|Обходимся без DL.|
|4|[Embeddings difference](https://github.com/khav-i/nlp_works/blob/master/Embeddings%20difference/README.md)|Смотрим на разницу эмбеддингов слов, генерируемых GloVe и BERT.|Инструменты flair.|
|5|[POS-tagging](https://github.com/khav-i/nlp_works/blob/master/POS-tagging/README.md)|Решаем классическую задачу POS-теггинга с помощью свёрточных сетей.|Ответ на вопрос — учитывать ли контекст.|
|6|[Word2Vec again](https://github.com/khav-i/nlp_works/blob/master/Word2Vec%20again/README.md)|В основе настоящей работы лежит [контестное задание из треннировок Яндекса по ML](https://github.com/girafe-ai/ml-course/blob/23f_ysda/homeworks/assignment14_word2vec/assignment_word2vec.ipynb).|Обучаем Skip-Gram Negative Sampling модель.|
|7|[Chars prediction (poetry generation)]()|В основе настоящей работы лежит [контестное задание из треннировок Яндекса по ML](https://github.com/girafe-ai/ml-course/blob/24f_yandex_ml_trainings/homeworks/hw02_attention_and_language_modeling/hw02_language_modeling.ipynb).|Обучаем GRU, AttentionRNN.|
